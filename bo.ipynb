{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import random, datetime, os\n",
    "\n",
    "# Gym is an OpenAI toolkit for RL\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Box\n",
    "from gymnasium.wrappers import FrameStack\n",
    "\n",
    "# Super Mario environment for OpenAI Gym\n",
    "\n",
    "from tensordict import TensorDict\n",
    "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3),\n",
      " 0.0,\n",
      " False,\n",
      " {'lives': 5, 'episode_frame_number': 2, 'frame_number': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = gym.make(\"Breakout-v4\", render_mode='rgb_array', obs_type=\"rgb\")\n",
    "\n",
    "\n",
    "env.reset()\n",
    "next_state, reward, done, trunc, info = env.step(action=0)\n",
    "print(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, SupportsFloat\n",
    "\n",
    "\n",
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        for i in range(self._skip):\n",
    "            # Accumulate reward and repeat the same action\n",
    "            obs, reward, done, trunk, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, trunk, info\n",
    "    \n",
    "    def render(self, *args, **kwargs):\n",
    "        return super().render(*args, **kwargs)\n",
    "\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape[:2]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def permute_orientation(self, observation):\n",
    "        # permute [H, W, C] array to [C, H, W] tensor\n",
    "        observation = np.transpose(observation, (2, 0, 1))\n",
    "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "        return observation\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = self.permute_orientation(observation)\n",
    "        transform = T.Grayscale()\n",
    "        observation = transform(observation)\n",
    "        return observation\n",
    "\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            self.shape = (shape, shape)\n",
    "        else:\n",
    "            self.shape = tuple(shape)\n",
    "\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transforms = T.Compose(\n",
    "            [T.Resize(self.shape, antialias=True), T.Normalize(0, 255)]\n",
    "        )\n",
    "        observation = transforms(observation).squeeze(0)\n",
    "        return observation\n",
    "    \n",
    "\n",
    "\n",
    "# Apply Wrappers to environment\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env)\n",
    "env = ResizeObservation(env, shape=84)\n",
    "if gym.__version__ < '0.26':\n",
    "    env = FrameStack(env, num_stack=4, new_step_api=True)\n",
    "else:\n",
    "    env = FrameStack(env, num_stack=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario:\n",
    "    def __init__(*args, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Given a state, choose an epsilon-greedy action\"\"\"\n",
    "        pass\n",
    "\n",
    "    def cache(self, experience):\n",
    "        \"\"\"Add the experience to memory\"\"\"\n",
    "        pass\n",
    "\n",
    "    def recall(self):\n",
    "        \"\"\"Sample experiences from memory\"\"\"\n",
    "        pass\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"Update online action value (Q) function with a batch of experiences\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class Mario:\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        # Mario's DNN to predict the most optimal action - we implement this in the Learn section\n",
    "        self.net = MarioNet(self.state_dim, self.action_dim).float()\n",
    "        self.net = self.net.to(device=self.device)\n",
    "\n",
    "        self.exploration_rate = 1\n",
    "        self.exploration_rate_decay = 0.99999975\n",
    "        self.exploration_rate_min = 0.1\n",
    "        self.curr_step = 0\n",
    "        self.current_episode = 0\n",
    "\n",
    "        self.save_every = 5e5  # no. of experiences between saving Mario Net\n",
    "    def set_episode(self, episode):\n",
    "        self.current_episode = episode\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "    Given a state, choose an epsilon-greedy action and update value of step.\n",
    "\n",
    "    Inputs:\n",
    "    state(``LazyFrame``): A single observation of the current state, dimension is (state_dim)\n",
    "    Outputs:\n",
    "    ``action_idx`` (``int``): An integer representing which action Mario will perform\n",
    "    \"\"\"\n",
    "        # EXPLORE\n",
    "        # Define the function to calculate epsilon\n",
    "        def epsilon(steps_done, EPS_START, EPS_END, DECAY_RATE):\n",
    "            return EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / DECAY_RATE)\n",
    "        eps = epsilon(self.current_episode, 1, 0.1, 25000)\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            action_idx = np.random.randint(self.action_dim)\n",
    "\n",
    "        # EXPLOIT\n",
    "        else:\n",
    "            state = state[0].__array__() if isinstance(state, tuple) else state.__array__()\n",
    "            state = torch.tensor(state, device=self.device).unsqueeze(0)\n",
    "            action_values = self.net(state, model=\"online\")\n",
    "            action_idx = torch.argmax(action_values, axis=1).item()\n",
    "\n",
    "        # decrease exploration_rate\n",
    "        self.exploration_rate *= self.exploration_rate_decay\n",
    "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
    "\n",
    "        # increment step\n",
    "        self.curr_step += 1\n",
    "        return action_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):  # subclassing for continuity\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.memory = TensorDictReplayBuffer(storage=LazyMemmapStorage(100000, device=torch.device(\"cpu\")))\n",
    "        self.batch_size = 32\n",
    "\n",
    "    def cache(self, state, next_state, action, reward, done):\n",
    "        \"\"\"\n",
    "        Store the experience to self.memory (replay buffer)\n",
    "\n",
    "        Inputs:\n",
    "        state (``LazyFrame``),\n",
    "        next_state (``LazyFrame``),\n",
    "        action (``int``),\n",
    "        reward (``float``),\n",
    "        done(``bool``))\n",
    "        \"\"\"\n",
    "        def first_if_tuple(x):\n",
    "            return x[0] if isinstance(x, tuple) else x\n",
    "        state = first_if_tuple(state).__array__()\n",
    "        next_state = first_if_tuple(next_state).__array__()\n",
    "\n",
    "        state = torch.tensor(state)\n",
    "        next_state = torch.tensor(next_state)\n",
    "        action = torch.tensor([action])\n",
    "        reward = torch.tensor([reward])\n",
    "        done = torch.tensor([done])\n",
    "\n",
    "        # self.memory.append((state, next_state, action, reward, done,))\n",
    "        self.memory.add(TensorDict({\"state\": state, \"next_state\": next_state, \"action\": action, \"reward\": reward, \"done\": done}, batch_size=[]))\n",
    "\n",
    "    def recall(self):\n",
    "        \"\"\"\n",
    "        Retrieve a batch of experiences from memory\n",
    "        \"\"\"\n",
    "        batch = self.memory.sample(self.batch_size).to(self.device)\n",
    "        state, next_state, action, reward, done = (batch.get(key) for key in (\"state\", \"next_state\", \"action\", \"reward\", \"done\"))\n",
    "        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarioNet(nn.Module):\n",
    "    \"\"\"mini CNN structure\n",
    "  input -> (conv2d + relu) x 3 -> flatten -> (dense + relu) x 2 -> output\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        c, h, w = input_dim\n",
    "\n",
    "        if h != 84:\n",
    "            raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
    "        if w != 84:\n",
    "            raise ValueError(f\"Expecting input width: 84, got: {w}\")\n",
    "\n",
    "        self.online = self.__build_cnn(c, output_dim)\n",
    "\n",
    "        self.target = self.__build_cnn(c, output_dim)\n",
    "        self.target.load_state_dict(self.online.state_dict())\n",
    "\n",
    "        # Q_target parameters are frozen.\n",
    "        for p in self.target.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, input, model):\n",
    "        if model == \"online\":\n",
    "            return self.online(input)\n",
    "        elif model == \"target\":\n",
    "            return self.target(input)\n",
    "\n",
    "    def __build_cnn(self, c, output_dim):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.gamma = 0.9\n",
    "\n",
    "    def td_estimate(self, state, action):\n",
    "        current_Q = self.net(state, model=\"online\")[\n",
    "            np.arange(0, self.batch_size), action\n",
    "        ]  # Q_online(s,a)\n",
    "        return current_Q\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def td_target(self, reward, next_state, done):\n",
    "        next_state_Q = self.net(next_state, model=\"online\")\n",
    "        best_action = torch.argmax(next_state_Q, axis=1)\n",
    "        next_Q = self.net(next_state, model=\"target\")[\n",
    "            np.arange(0, self.batch_size), best_action\n",
    "        ]\n",
    "        return (reward + (1 - done.float()) * self.gamma * next_Q).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
    "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
    "\n",
    "    def update_Q_online(self, td_estimate, td_target):\n",
    "        loss = self.loss_fn(td_estimate, td_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def sync_Q_target(self):\n",
    "        self.net.target.load_state_dict(self.net.online.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def save(self):\n",
    "        save_path = (\n",
    "            self.save_dir / f\"mario_net_{int(self.curr_step // self.save_every)}.chkpt\"\n",
    "        )\n",
    "        torch.save(\n",
    "            dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate),\n",
    "            save_path,\n",
    "        )\n",
    "        print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        \n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.burnin = 1e4  # min. experiences before training\n",
    "        self.learn_every = 3  # no. of experiences between updates to Q_online\n",
    "        self.sync_every = 1e4  # no. of experiences between Q_target & Q_online sync\n",
    "\n",
    "    def learn(self):\n",
    "        if self.curr_step % self.sync_every == 0:\n",
    "            self.sync_Q_target()\n",
    "\n",
    "        if self.curr_step % self.save_every == 0:\n",
    "            self.save()\n",
    "\n",
    "        if self.curr_step < self.burnin:\n",
    "            return None, None\n",
    "\n",
    "        if self.curr_step % self.learn_every != 0:\n",
    "            return None, None\n",
    "\n",
    "        # Sample from memory\n",
    "        state, next_state, action, reward, done = self.recall()\n",
    "\n",
    "        # Get TD Estimate\n",
    "        td_est = self.td_estimate(state, action)\n",
    "\n",
    "        # Get TD Target\n",
    "        td_tgt = self.td_target(reward, next_state, done)\n",
    "\n",
    "        # Backpropagate loss through Q_online\n",
    "        loss = self.update_Q_online(td_est, td_tgt)\n",
    "\n",
    "        return (td_est.mean().item(), loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time, datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class MetricLogger:\n",
    "    def __init__(self, save_dir):\n",
    "        self.save_log = save_dir / \"log\"\n",
    "        with open(self.save_log, \"w\") as f:\n",
    "            f.write(\n",
    "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
    "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
    "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
    "            )\n",
    "        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n",
    "        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
    "        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n",
    "        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n",
    "\n",
    "        # History metrics\n",
    "        self.ep_rewards = []\n",
    "        self.ep_lengths = []\n",
    "        self.ep_avg_losses = []\n",
    "        self.ep_avg_qs = []\n",
    "\n",
    "        # Moving averages, added for every call to record()\n",
    "        self.moving_avg_ep_rewards = []\n",
    "        self.moving_avg_ep_lengths = []\n",
    "        self.moving_avg_ep_avg_losses = []\n",
    "        self.moving_avg_ep_avg_qs = []\n",
    "\n",
    "        # Current episode metric\n",
    "        self.init_episode()\n",
    "\n",
    "        # Timing\n",
    "        self.record_time = time.time()\n",
    "\n",
    "    def log_step(self, reward, loss, q):\n",
    "        self.curr_ep_reward += reward\n",
    "        self.curr_ep_length += 1\n",
    "        if loss:\n",
    "            self.curr_ep_loss += loss\n",
    "            self.curr_ep_q += q\n",
    "            self.curr_ep_loss_length += 1\n",
    "\n",
    "    def log_episode(self):\n",
    "        \"Mark end of episode\"\n",
    "        self.ep_rewards.append(self.curr_ep_reward)\n",
    "        self.ep_lengths.append(self.curr_ep_length)\n",
    "        if self.curr_ep_loss_length == 0:\n",
    "            ep_avg_loss = 0\n",
    "            ep_avg_q = 0\n",
    "        else:\n",
    "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
    "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
    "        self.ep_avg_losses.append(ep_avg_loss)\n",
    "        self.ep_avg_qs.append(ep_avg_q)\n",
    "\n",
    "        self.init_episode()\n",
    "\n",
    "    def init_episode(self):\n",
    "        self.curr_ep_reward = 0.0\n",
    "        self.curr_ep_length = 0\n",
    "        self.curr_ep_loss = 0.0\n",
    "        self.curr_ep_q = 0.0\n",
    "        self.curr_ep_loss_length = 0\n",
    "\n",
    "    def record(self, episode, epsilon, step):\n",
    "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
    "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
    "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
    "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
    "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
    "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
    "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
    "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
    "\n",
    "        last_record_time = self.record_time\n",
    "        self.record_time = time.time()\n",
    "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
    "\n",
    "        print(\n",
    "            f\"Episode {episode} - \"\n",
    "            f\"Step {step} - \"\n",
    "            f\"Epsilon {epsilon} - \"\n",
    "            f\"Mean Reward {mean_ep_reward} - \"\n",
    "            f\"Mean Length {mean_ep_length} - \"\n",
    "            f\"Mean Loss {mean_ep_loss} - \"\n",
    "            f\"Mean Q Value {mean_ep_q} - \"\n",
    "            f\"Time Delta {time_since_last_record} - \"\n",
    "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
    "        )\n",
    "\n",
    "        with open(self.save_log, \"a\") as f:\n",
    "            f.write(\n",
    "                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
    "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
    "                f\"{time_since_last_record:15.3f}\"\n",
    "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
    "            )\n",
    "\n",
    "        for metric in [\"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\", \"ep_rewards\"]:\n",
    "            plt.clf()\n",
    "            plt.plot(getattr(self, f\"moving_avg_{metric}\"), label=f\"moving_avg_{metric}\")\n",
    "            plt.legend()\n",
    "            plt.savefig(getattr(self, f\"{metric}_plot\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: False\n",
      "\n",
      "Episode 0 - Step 49 - Epsilon 0.999987750073498 - Mean Reward 0.0 - Mean Length 49.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.355 - Time 2024-05-12T16:59:41\n",
      "Episode 20 - Step 1494 - Epsilon 0.999626569695719 - Mean Reward 1.238 - Mean Length 71.143 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 5.807 - Time 2024-05-12T16:59:47\n",
      "Episode 40 - Step 2873 - Epsilon 0.9992820077899693 - Mean Reward 1.171 - Mean Length 70.073 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 5.178 - Time 2024-05-12T16:59:52\n",
      "Episode 60 - Step 4420 - Epsilon 0.9988956101495641 - Mean Reward 1.344 - Mean Length 72.459 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 5.663 - Time 2024-05-12T16:59:58\n",
      "Episode 80 - Step 6047 - Epsilon 0.9984893919295325 - Mean Reward 1.506 - Mean Length 74.654 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 5.951 - Time 2024-05-12T17:00:04\n",
      "Episode 100 - Step 7430 - Epsilon 0.9981442238534465 - Mean Reward 1.48 - Mean Length 73.81 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 5.384 - Time 2024-05-12T17:00:09\n",
      "Episode 120 - Step 8860 - Epsilon 0.9977874510257134 - Mean Reward 1.49 - Mean Length 73.66 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 5.41 - Time 2024-05-12T17:00:15\n",
      "Episode 140 - Step 10290 - Epsilon 0.9974308057214877 - Mean Reward 1.5 - Mean Length 74.17 - Mean Loss 0.001 - Mean Q Value 0.001 - Time Delta 9.647 - Time 2024-05-12T17:00:24\n",
      "Episode 160 - Step 11751 - Epsilon 0.9970665605984325 - Mean Reward 1.41 - Mean Length 73.31 - Mean Loss 0.002 - Mean Q Value 0.005 - Time Delta 26.04 - Time 2024-05-12T17:00:50\n",
      "Episode 180 - Step 13322 - Epsilon 0.9966750395477411 - Mean Reward 1.38 - Mean Length 72.75 - Mean Loss 0.005 - Mean Q Value 0.007 - Time Delta 29.344 - Time 2024-05-12T17:01:20\n",
      "Episode 200 - Step 14801 - Epsilon 0.9963065870276175 - Mean Reward 1.39 - Mean Length 73.71 - Mean Loss 0.006 - Mean Q Value 0.011 - Time Delta 29.115 - Time 2024-05-12T17:01:49\n",
      "Episode 220 - Step 16277 - Epsilon 0.9959390176717238 - Mean Reward 1.41 - Mean Length 74.17 - Mean Loss 0.008 - Mean Q Value 0.017 - Time Delta 28.648 - Time 2024-05-12T17:02:17\n",
      "Episode 240 - Step 17823 - Epsilon 0.9955541615714656 - Mean Reward 1.55 - Mean Length 75.33 - Mean Loss 0.01 - Mean Q Value 0.017 - Time Delta 28.051 - Time 2024-05-12T17:02:45\n",
      "Episode 260 - Step 19311 - Epsilon 0.9951838842528749 - Mean Reward 1.64 - Mean Length 75.6 - Mean Loss 0.01 - Mean Q Value 0.013 - Time Delta 26.893 - Time 2024-05-12T17:03:12\n",
      "Episode 280 - Step 20871 - Epsilon 0.9947958381633623 - Mean Reward 1.6 - Mean Length 75.49 - Mean Loss 0.01 - Mean Q Value 0.014 - Time Delta 28.332 - Time 2024-05-12T17:03:41\n",
      "Episode 300 - Step 22565 - Epsilon 0.9943746312700451 - Mean Reward 1.74 - Mean Length 77.64 - Mean Loss 0.01 - Mean Q Value 0.016 - Time Delta 32.453 - Time 2024-05-12T17:04:13\n",
      "Episode 320 - Step 24047 - Epsilon 0.9940062836636714 - Mean Reward 1.74 - Mean Length 77.7 - Mean Loss 0.01 - Mean Q Value 0.017 - Time Delta 27.505 - Time 2024-05-12T17:04:41\n",
      "Episode 340 - Step 25584 - Epsilon 0.9936244100735477 - Mean Reward 1.7 - Mean Length 77.61 - Mean Loss 0.01 - Mean Q Value 0.022 - Time Delta 28.82 - Time 2024-05-12T17:05:09\n",
      "Episode 360 - Step 26978 - Epsilon 0.9932781922552715 - Mean Reward 1.6 - Mean Length 76.67 - Mean Loss 0.01 - Mean Q Value 0.028 - Time Delta 26.993 - Time 2024-05-12T17:05:36\n",
      "Episode 380 - Step 28510 - Epsilon 0.9928978395022436 - Mean Reward 1.62 - Mean Length 76.39 - Mean Loss 0.01 - Mean Q Value 0.031 - Time Delta 29.172 - Time 2024-05-12T17:06:06\n",
      "Episode 400 - Step 29958 - Epsilon 0.992538475488184 - Mean Reward 1.49 - Mean Length 73.93 - Mean Loss 0.01 - Mean Q Value 0.031 - Time Delta 28.119 - Time 2024-05-12T17:06:34\n",
      "Episode 420 - Step 31323 - Epsilon 0.9921998294758076 - Mean Reward 1.45 - Mean Length 72.76 - Mean Loss 0.01 - Mean Q Value 0.033 - Time Delta 26.153 - Time 2024-05-12T17:07:00\n",
      "Episode 440 - Step 32750 - Epsilon 0.9918459252737688 - Mean Reward 1.41 - Mean Length 71.66 - Mean Loss 0.01 - Mean Q Value 0.036 - Time Delta 27.031 - Time 2024-05-12T17:07:27\n",
      "Episode 460 - Step 34334 - Epsilon 0.9914532319966187 - Mean Reward 1.55 - Mean Length 73.56 - Mean Loss 0.01 - Mean Q Value 0.039 - Time Delta 30.127 - Time 2024-05-12T17:07:57\n",
      "Episode 480 - Step 35893 - Epsilon 0.9910668883447058 - Mean Reward 1.57 - Mean Length 73.83 - Mean Loss 0.01 - Mean Q Value 0.042 - Time Delta 29.659 - Time 2024-05-12T17:08:27\n",
      "Episode 500 - Step 37302 - Epsilon 0.9907178464682169 - Mean Reward 1.51 - Mean Length 73.44 - Mean Loss 0.01 - Mean Q Value 0.044 - Time Delta 26.873 - Time 2024-05-12T17:08:53\n",
      "Episode 520 - Step 38771 - Epsilon 0.9903540720957364 - Mean Reward 1.58 - Mean Length 74.48 - Mean Loss 0.011 - Mean Q Value 0.045 - Time Delta 28.535 - Time 2024-05-12T17:09:22\n",
      "Episode 540 - Step 40295 - Epsilon 0.9899768190183336 - Mean Reward 1.67 - Mean Length 75.45 - Mean Loss 0.011 - Mean Q Value 0.046 - Time Delta 29.856 - Time 2024-05-12T17:09:52\n",
      "Episode 560 - Step 41799 - Epsilon 0.9896046576585307 - Mean Reward 1.6 - Mean Length 74.65 - Mean Loss 0.011 - Mean Q Value 0.05 - Time Delta 29.685 - Time 2024-05-12T17:10:22\n",
      "Episode 580 - Step 43236 - Epsilon 0.9892492059926103 - Mean Reward 1.47 - Mean Length 73.43 - Mean Loss 0.011 - Mean Q Value 0.053 - Time Delta 28.271 - Time 2024-05-12T17:10:50\n",
      "Episode 600 - Step 44630 - Epsilon 0.9889045126674877 - Mean Reward 1.49 - Mean Length 73.28 - Mean Loss 0.01 - Mean Q Value 0.056 - Time Delta 26.924 - Time 2024-05-12T17:11:17\n",
      "Episode 620 - Step 46113 - Epsilon 0.9885379442303679 - Mean Reward 1.43 - Mean Length 73.42 - Mean Loss 0.01 - Mean Q Value 0.059 - Time Delta 28.703 - Time 2024-05-12T17:11:45\n",
      "Episode 640 - Step 47728 - Epsilon 0.98813890254741 - Mean Reward 1.45 - Mean Length 74.33 - Mean Loss 0.01 - Mean Q Value 0.062 - Time Delta 30.752 - Time 2024-05-12T17:12:16\n",
      "Episode 660 - Step 49188 - Epsilon 0.9877782976172572 - Mean Reward 1.36 - Mean Length 73.89 - Mean Loss 0.01 - Mean Q Value 0.062 - Time Delta 28.457 - Time 2024-05-12T17:12:45\n",
      "Episode 680 - Step 50698 - Epsilon 0.9874054816366987 - Mean Reward 1.44 - Mean Length 74.62 - Mean Loss 0.01 - Mean Q Value 0.062 - Time Delta 29.949 - Time 2024-05-12T17:13:15\n",
      "Episode 700 - Step 52209 - Epsilon 0.9870325596094254 - Mean Reward 1.54 - Mean Length 75.79 - Mean Loss 0.01 - Mean Q Value 0.064 - Time Delta 29.497 - Time 2024-05-12T17:13:44\n",
      "Episode 720 - Step 53733 - Epsilon 0.9866565717873907 - Mean Reward 1.58 - Mean Length 76.2 - Mean Loss 0.01 - Mean Q Value 0.067 - Time Delta 29.705 - Time 2024-05-12T17:14:14\n",
      "Episode 740 - Step 55279 - Epsilon 0.9862753026596878 - Mean Reward 1.55 - Mean Length 75.51 - Mean Loss 0.01 - Mean Q Value 0.068 - Time Delta 30.398 - Time 2024-05-12T17:14:44\n",
      "Episode 760 - Step 56735 - Epsilon 0.9859163637354489 - Mean Reward 1.62 - Mean Length 75.47 - Mean Loss 0.01 - Mean Q Value 0.071 - Time Delta 28.943 - Time 2024-05-12T17:15:13\n",
      "Episode 780 - Step 58167 - Epsilon 0.9855634688050314 - Mean Reward 1.57 - Mean Length 74.69 - Mean Loss 0.01 - Mean Q Value 0.072 - Time Delta 28.453 - Time 2024-05-12T17:15:42\n",
      "Episode 800 - Step 59648 - Epsilon 0.9851986314297381 - Mean Reward 1.51 - Mean Length 74.39 - Mean Loss 0.01 - Mean Q Value 0.072 - Time Delta 27.924 - Time 2024-05-12T17:16:10\n",
      "Episode 820 - Step 61216 - Epsilon 0.9848125092027997 - Mean Reward 1.58 - Mean Length 74.83 - Mean Loss 0.01 - Mean Q Value 0.074 - Time Delta 29.336 - Time 2024-05-12T17:16:39\n",
      "Episode 840 - Step 62789 - Epsilon 0.9844253077735726 - Mean Reward 1.57 - Mean Length 75.1 - Mean Loss 0.01 - Mean Q Value 0.076 - Time Delta 29.711 - Time 2024-05-12T17:17:09\n",
      "Episode 860 - Step 64169 - Epsilon 0.9840857395787813 - Mean Reward 1.51 - Mean Length 74.34 - Mean Loss 0.01 - Mean Q Value 0.079 - Time Delta 26.466 - Time 2024-05-12T17:17:35\n",
      "Episode 880 - Step 65715 - Epsilon 0.9837054638858574 - Mean Reward 1.55 - Mean Length 75.48 - Mean Loss 0.01 - Mean Q Value 0.082 - Time Delta 29.686 - Time 2024-05-12T17:18:05\n",
      "Episode 900 - Step 67283 - Epsilon 0.9833199268659616 - Mean Reward 1.64 - Mean Length 76.35 - Mean Loss 0.01 - Mean Q Value 0.085 - Time Delta 31.151 - Time 2024-05-12T17:18:36\n",
      "Episode 920 - Step 68938 - Epsilon 0.9829131623505314 - Mean Reward 1.72 - Mean Length 77.22 - Mean Loss 0.01 - Mean Q Value 0.087 - Time Delta 32.198 - Time 2024-05-12T17:19:08\n",
      "Episode 940 - Step 70403 - Epsilon 0.982553236275265 - Mean Reward 1.64 - Mean Length 76.14 - Mean Loss 0.011 - Mean Q Value 0.088 - Time Delta 27.892 - Time 2024-05-12T17:19:36\n",
      "Episode 960 - Step 71758 - Epsilon 0.9822204526932357 - Mean Reward 1.69 - Mean Length 75.89 - Mean Loss 0.011 - Mean Q Value 0.09 - Time Delta 27.234 - Time 2024-05-12T17:20:03\n",
      "Episode 980 - Step 73235 - Epsilon 0.981857834698168 - Mean Reward 1.67 - Mean Length 75.2 - Mean Loss 0.011 - Mean Q Value 0.091 - Time Delta 30.468 - Time 2024-05-12T17:20:34\n",
      "Episode 1000 - Step 74751 - Epsilon 0.9814857810408807 - Mean Reward 1.65 - Mean Length 74.68 - Mean Loss 0.011 - Mean Q Value 0.094 - Time Delta 30.059 - Time 2024-05-12T17:21:04\n",
      "Episode 1020 - Step 76299 - Epsilon 0.9811060194846954 - Mean Reward 1.54 - Mean Length 73.61 - Mean Loss 0.01 - Mean Q Value 0.095 - Time Delta 30.644 - Time 2024-05-12T17:21:34\n",
      "Episode 1040 - Step 77944 - Epsilon 0.9807026225378832 - Mean Reward 1.62 - Mean Length 75.41 - Mean Loss 0.01 - Mean Q Value 0.097 - Time Delta 31.108 - Time 2024-05-12T17:22:05\n",
      "Episode 1060 - Step 79333 - Epsilon 0.9803621326305801 - Mean Reward 1.61 - Mean Length 75.75 - Mean Loss 0.01 - Mean Q Value 0.097 - Time Delta 28.244 - Time 2024-05-12T17:22:34\n",
      "Episode 1080 - Step 80816 - Epsilon 0.9799987306942277 - Mean Reward 1.61 - Mean Length 75.81 - Mean Loss 0.011 - Mean Q Value 0.099 - Time Delta 30.299 - Time 2024-05-12T17:23:04\n",
      "Episode 1100 - Step 82250 - Epsilon 0.9796474640736201 - Mean Reward 1.56 - Mean Length 74.99 - Mean Loss 0.011 - Mean Q Value 0.101 - Time Delta 31.206 - Time 2024-05-12T17:23:35\n",
      "Episode 1120 - Step 83716 - Epsilon 0.9792884890193115 - Mean Reward 1.52 - Mean Length 74.17 - Mean Loss 0.01 - Mean Q Value 0.103 - Time Delta 32.882 - Time 2024-05-12T17:24:08\n",
      "Episode 1140 - Step 85098 - Epsilon 0.9789502032462543 - Mean Reward 1.4 - Mean Length 71.54 - Mean Loss 0.01 - Mean Q Value 0.105 - Time Delta 26.644 - Time 2024-05-12T17:24:35\n",
      "Episode 1160 - Step 86602 - Epsilon 0.9785821871151512 - Mean Reward 1.44 - Mean Length 72.69 - Mean Loss 0.01 - Mean Q Value 0.107 - Time Delta 27.983 - Time 2024-05-12T17:25:03\n",
      "Episode 1180 - Step 88188 - Epsilon 0.9781942561418125 - Mean Reward 1.52 - Mean Length 73.72 - Mean Loss 0.01 - Mean Q Value 0.109 - Time Delta 30.681 - Time 2024-05-12T17:25:33\n",
      "Episode 1200 - Step 89695 - Epsilon 0.9778257908237146 - Mean Reward 1.51 - Mean Length 74.45 - Mean Loss 0.01 - Mean Q Value 0.108 - Time Delta 28.776 - Time 2024-05-12T17:26:02\n",
      "Episode 1220 - Step 91271 - Epsilon 0.9774406033008518 - Mean Reward 1.55 - Mean Length 75.55 - Mean Loss 0.01 - Mean Q Value 0.11 - Time Delta 30.325 - Time 2024-05-12T17:26:33\n",
      "Episode 1240 - Step 92716 - Epsilon 0.9770875666098223 - Mean Reward 1.56 - Mean Length 76.18 - Mean Loss 0.011 - Mean Q Value 0.112 - Time Delta 30.436 - Time 2024-05-12T17:27:03\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(f\"Using CUDA: {use_cuda}\")\n",
    "print()\n",
    "\n",
    "save_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "save_dir.mkdir(parents=True)\n",
    "\n",
    "mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)\n",
    "\n",
    "logger = MetricLogger(save_dir)\n",
    "\n",
    "episodes = 100_000\n",
    "for e in range(episodes):\n",
    "\n",
    "    state = env.reset(seed=420)\n",
    "\n",
    "    # Play the game!\n",
    "    while True:\n",
    "        mario.set_episode(e)\n",
    "\n",
    "        # Run agent on the state\n",
    "        action = mario.act(state)\n",
    "\n",
    "        # Agent performs action\n",
    "        next_state, reward, done, trunc, info = env.step(action)\n",
    "\n",
    "        # Remember\n",
    "        mario.cache(state, next_state, action, reward, done)\n",
    "\n",
    "        # Learn\n",
    "        q, loss = mario.learn()\n",
    "\n",
    "        # Logging\n",
    "        logger.log_step(reward, loss, q)\n",
    "\n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "        # Check if end of game\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    logger.log_episode()\n",
    "\n",
    "    if (e % 20 == 0) or (e == episodes - 1):\n",
    "        logger.record(episode=e, epsilon=mario.exploration_rate, step=mario.curr_step)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MarioNet saved to checkpoints/2024-05-12T16-55-01/mario_net_0.chkpt at step 65\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "mario.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nix/store/2kr0sgrp3xrq0279bfnifdhhhv12d8v6-python3.11-gymnasium-0.29.1/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:335: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n",
      "1   HIToolbox                           0x000000019b58c5c8 _ZN15MenuBarInstance22EnsureAutoShowObserverEv + 120\n",
      "2   HIToolbox                           0x000000019b58c188 _ZN15MenuBarInstance14EnableAutoShowEv + 60\n",
      "3   HIToolbox                           0x000000019b52f310 SetMenuBarObscured + 372\n",
      "4   HIToolbox                           0x000000019b52eee8 _ZN13HIApplication15HandleActivatedEP14OpaqueEventRefhP15OpaqueWindowPtrh + 172\n",
      "5   HIToolbox                           0x000000019b528fcc _ZN13HIApplication13EventObserverEjP14OpaqueEventRefPv + 296\n",
      "6   HIToolbox                           0x000000019b4efcd0 _NotifyEventLoopObservers + 176\n",
      "7   HIToolbox                           0x000000019b52896c AcquireEventFromQueue + 432\n",
      "8   HIToolbox                           0x000000019b517c84 ReceiveNextEventCommon + 320\n",
      "9   HIToolbox                           0x000000019b517b2c _BlockUntilNextEventMatchingListInModeWithFilter + 72\n",
      "10  AppKit                              0x00000001950bd84c _DPSNextEvent + 632\n",
      "11  AppKit                              0x00000001950bc9dc -[NSApplication(NSEvent) _nextEventMatchingEventMask:untilDate:inMode:dequeue:] + 728\n",
      "12  libSDL2-2.0.dylib                   0x000000015de38fb4 Cocoa_PumpEvents + 172\n",
      "13  libSDL2-2.0.dylib                   0x000000015dd9a40c SDL_WaitEventTimeout_REAL + 468\n",
      "14  _ale_py.cpython-311-darwin.so       0x00000001470da350 _ZN3ale9ScreenSDL6renderEv + 304\n",
      "15  _ale_py.cpython-311-darwin.so       0x0000000147136640 _ZN3ale17StellaEnvironment3actENS_6ActionES1_ + 536\n",
      "16  _ale_py.cpython-311-darwin.so       0x0000000147180130 _ZZN8pybind1112cpp_function10initializeIZNS0_C1IiN3ale18ALEPythonInterfaceEJNS3_6ActionEEJNS_4nameENS_9is_methodENS_7siblingEEEEMT0_FT_DpT1_EDpRKT2_EUlPS4_S5_E_iJSJ_S5_EJS6_S7_S8_EEEvOSA_PFS9_SC_ESI_ENUlRNS_6detail13function_callEE_8__invokeESQ_ + 184\n",
      "17  _ale_py.cpython-311-darwin.so       0x000000014716d2cc _ZN8pybind1112cpp_function10dispatcherEP7_objectS2_S2_ + 3784\n",
      "18  libpython3.11.dylib                 0x0000000101464cd0 cfunction_call + 84\n",
      "19  libpython3.11.dylib                 0x0000000101418640 _PyObject_MakeTpCall + 344\n",
      "20  libpython3.11.dylib                 0x00000001014fc554 _PyEval_EvalFrameDefault + 34988\n",
      "21  libpython3.11.dylib                 0x00000001014f3abc PyEval_EvalCode + 240\n",
      "22  libpython3.11.dylib                 0x00000001014f0438 builtin_exec + 916\n",
      "23  libpython3.11.dylib                 0x00000001014fd3d0 _PyEval_EvalFrameDefault + 38696\n",
      "24  libpython3.11.dylib                 0x0000000101431f94 gen_send_ex2 + 632\n",
      "25  libpython3.11.dylib                 0x00000001014f6368 _PyEval_EvalFrameDefault + 9920\n",
      "26  libpython3.11.dylib                 0x0000000101431f94 gen_send_ex2 + 632\n",
      "27  libpython3.11.dylib                 0x00000001014f6368 _PyEval_EvalFrameDefault + 9920\n",
      "28  libpython3.11.dylib                 0x0000000101431f94 gen_send_ex2 + 632\n",
      "29  libpython3.11.dylib                 0x0000000101431bc0 gen_send_ex + 56\n",
      "30  libpython3.11.dylib                 0x00000001014fd95c _PyEval_EvalFrameDefault + 40116\n",
      "31  libpython3.11.dylib                 0x00000001014f3c08 _PyEval_Vector + 200\n",
      "32  libpython3.11.dylib                 0x000000010141b2e0 method_vectorcall + 176\n",
      "33  libpython3.11.dylib                 0x00000001014189f0 _PyVectorcall_Call + 132\n",
      "34  libpython3.11.dylib                 0x00000001014fe094 _PyEval_EvalFrameDefault + 41964\n",
      "35  libpython3.11.dylib                 0x0000000101431f94 gen_send_ex2 + 632\n",
      "36  libpython3.11.dylib                 0x00000001014f6368 _PyEval_EvalFrameDefault + 9920\n",
      "37  libpython3.11.dylib                 0x0000000101431f94 gen_send_ex2 + 632\n",
      "38  libpython3.11.dylib                 0x00000001014f6368 _PyEval_EvalFrameDefault + 9920\n",
      "39  libpython3.11.dylib                 0x0000000101431f94 gen_send_ex2 + 632\n",
      "40  libpython3.11.dylib                 0x00000001014f6368 _PyEval_EvalFrameDefault + 9920\n",
      "41  libpython3.11.dylib                 0x0000000101431f94 gen_send_ex2 + 632\n",
      "42  libpython3.11.dylib                 0x00000001014f6368 _PyEval_EvalFrameDefault + 9920\n",
      "43  libpython3.11.dylib                 0x0000000101431f94 gen_send_ex2 + 632\n",
      "44  libpython3.11.dylib                 0x00000001014f6368 _PyEval_EvalFrameDefault + 9920\n",
      "45  libpython3.11.dylib                 0x0000000101431f94 gen_send_ex2 + 632\n",
      "46  _asyncio.cpython-311-darwin.so      0x0000000101f98c58 task_step + 460\n",
      "47  _asyncio.cpython-311-darwin.so      0x0000000101f9a078 task_wakeup + 136\n",
      "48  libpython3.11.dylib                 0x00000001014647c8 cfunction_vectorcall_O + 332\n",
      "49  libpython3.11.dylib                 0x000000010151d3bc context_run + 288\n",
      "50  libpython3.11.dylib                 0x00000001014644f8 cfunction_vectorcall_FASTCALL_KEYWORDS + 160\n",
      "51  libpython3.11.dylib                 0x00000001014fe094 _PyEval_EvalFrameDefault + 41964\n",
      "52  libpython3.11.dylib                 0x00000001014f3abc PyEval_EvalCode + 240\n",
      "53  libpython3.11.dylib                 0x00000001014f0438 builtin_exec + 916\n",
      "54  libpython3.11.dylib                 0x00000001014644f8 cfunction_vectorcall_FASTCALL_KEYWORDS + 160\n",
      "55  libpython3.11.dylib                 0x0000000101418b68 PyObject_Vectorcall + 80\n",
      "56  libpython3.11.dylib                 0x00000001014fc554 _PyEval_EvalFrameDefault + 34988\n",
      "57  libpython3.11.dylib                 0x00000001014f3c08 _PyEval_Vector + 200\n",
      "58  libpython3.11.dylib                 0x00000001015670dc pymain_run_module + 216\n",
      "59  libpython3.11.dylib                 0x000000010156691c Py_RunMain + 1348\n",
      "60  libpython3.11.dylib                 0x0000000101566f2c pymain_main + 324\n",
      "61  libpython3.11.dylib                 0x0000000101566fdc Py_BytesMain + 56\n",
      "62  dyld                                0x0000000191a2fe50 start + 2544\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# checkpoints/2024-05-12T16-24-59/mario_net_0.chkpt\n",
    "# load the model and play the game\n",
    "\n",
    "from pathlib import Path\n",
    "# load the model\n",
    "save_dir = Path(\"checkpoints/2024-05-12T16-31-07\")\n",
    "\n",
    "mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)\n",
    "mario.net.load_state_dict(torch.load(save_dir / \"mario_net_0.chkpt\")[\"model\"])\n",
    "mario.net.eval()\n",
    "\n",
    "env = gym.make(\"Breakout-v4\", render_mode='human', obs_type=\"rgb\")\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env)\n",
    "env = ResizeObservation(env, shape=84)\n",
    "\n",
    "# Play the game\n",
    "state = env.reset(seed=420)\n",
    "while True:\n",
    "    env.render()\n",
    "    action = mario.act(state)\n",
    "    next_state, reward, done, _, _ = env.step(action)\n",
    "    state = next_state\n",
    "    if done:\n",
    "        break\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
